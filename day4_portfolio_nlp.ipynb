{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Portfolio DAY 4 - Natural Language Processing\n",
    "## Fake Trump Twitter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_json('./data/trump_vs_GPT2.gz')\n",
    "df.rename(columns = { 0: 'tweet', 1: 'real_tweet' }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\caspe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\caspe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenized = []\n",
    "\n",
    "for index, tweet in enumerate(df['tweet'].values):\n",
    "    tokens = [word.lower() for word in word_tokenize(tweet) if word not in stop_words and word.isalnum()]\n",
    "    tokenized.append(' '.join(tokens))\n",
    "\n",
    "df['tweet_token'] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the text\n",
    "vectorizer_bow = CountVectorizer()\n",
    "vectorizer_tfid = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask to ensure same training sample\n",
    "indices_all = np.arange(0, len(df), 1, dtype=int) # Create a range of all the indices\n",
    "np.random.shuffle(indices_all) # Shuffle the indices\n",
    "\n",
    "test_size = floor(len(indices_all) * 0.33)\n",
    "test_mask = indices_all[0:test_size]\n",
    "\n",
    "train_size = len(indices_all) - test_size\n",
    "train_mask = indices_all[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the text\n",
    "vectorizer_bow = CountVectorizer()\n",
    "vectorizer_tfid = TfidfVectorizer()\n",
    "\n",
    "y_test = df.iloc[test_mask]['real_tweet'].values\n",
    "y_train = df.iloc[train_mask]['real_tweet'].values\n",
    "\n",
    "X_test = df.iloc[test_mask]['tweet_token'].values\n",
    "X_train = df.iloc[train_mask]['tweet_token'].values\n",
    "\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
    "X_train_tfid = vectorizer_tfid.fit_transform(X_train)\n",
    "\n",
    "X_test_bow = vectorizer_bow.transform(X_test)\n",
    "X_test_tfid = vectorizer_tfid.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bow: 0.81, tfid: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Fit first attempt to assess accuracy\n",
    "model_bow = LogisticRegression(max_iter=2000)\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "bow_score = model_bow.score(X_test_bow, y_test)\n",
    "\n",
    "model_tfid = LogisticRegression(max_iter=2000)\n",
    "model_tfid.fit(X_train_tfid, y_train)\n",
    "tfid_score = model_tfid.score(X_test_tfid, y_test)\n",
    "\n",
    "print(f'bow: {round(bow_score, 2)}, tfid: {round(tfid_score, 2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 126 candidates, totalling 378 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   10.0s\n",
      "0.8272315919374743\n",
      "{'tol': 0.001, 'solver': 'newton-cg', 'max_iter': 100, 'C': 0.1}\n",
      "[Parallel(n_jobs=-1)]: Done 378 out of 378 | elapsed:   12.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Model tuning parameters\n",
    "max_iter = [100, 500, 1000, 2000]\n",
    "solver = ['newton-cg', 'lbfgs']\n",
    "C = [0.05, 0.1, 0.15, 1.0, 1.5, 2, 10]\n",
    "tol = [0.001, 0.005, 0.010]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = { 'max_iter': max_iter, 'solver': solver, 'C': C, 'tol': tol }\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier_random = RandomizedSearchCV(estimator = classifier, param_distributions = random_grid, n_iter = 300, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "classifier_random.fit(X_train_bow, y_train)\n",
    "print(classifier_random.score(X_test_bow, y_test))\n",
    "print(classifier_random.best_params_)\n",
    "\n",
    "classifier_random.fit(X_train_tfid, y_train)\n",
    "print(classifier_random.score(X_test_tfid, y_test))\n",
    "print(classifier_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bow: 0.83, tfid: 0.81\n"
     ]
    }
   ],
   "source": [
    "# bow {'tol': 0.001, 'solver': 'newton-cg', 'max_iter': 100, 'C': 0.1}\n",
    "# tfid {'tol': 0.001, 'solver': 'newton-cg', 'max_iter': 100, 'C': 1.0}\n",
    "\n",
    "model_bow = LogisticRegression(tol=0.001, solver='newton-cg', max_iter=100, C=0.1)\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "bow_score = model_bow.score(X_test_bow, y_test)\n",
    "\n",
    "model_tfid = LogisticRegression(tol=0.001, solver='newton-cg', max_iter=100, C=1.0)\n",
    "model_tfid.fit(X_train_tfid, y_train)\n",
    "tfid_score = model_tfid.score(X_test_tfid, y_test)\n",
    "\n",
    "print(f'bow: {round(bow_score, 2)}, tfid: {round(tfid_score, 2)}')\n"
   ]
  },
  {
   "source": [
    "## Conclusion\n",
    "Slight improvement in bag of words approach.\n",
    "0.83% accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}